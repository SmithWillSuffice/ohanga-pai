<!doctype html><html lang=en data-mode=dark><head prefix="og: http://ogp.me/ns#"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.101.0"><meta name=theme content="Color Your World -- gitlab.com/rmaguiar/hugo-theme-color-your-world"><title>Statistical Knowing | Ōhanga Pai</title><meta name=author content="Bijou M. Smith"><meta name=robots content="index follow"><link rel=canonical href=https://smithwillsuffice.github.io/blog/26_statsknowing/><meta property="og:site_name" content="Ōhanga Pai"><meta property="og:title" content="Statistical Knowing"><meta property="og:url" content="https://smithwillsuffice.github.io/blog/26_statsknowing/"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-05-10"><meta property="article:modified_time" content="2023-05-10"><meta property="og:updated_time" content="2023-05-10"><meta name=twitter:dnt content="on"><meta name=theme-color content="#222"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="default"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebSite","@id":"https://smithwillsuffice.github.io/"},"headline":"Statistical Knowing","description":"","url":"https://smithwillsuffice.github.io/blog/26_statsknowing/","inLanguage":"en","datePublished":"2023-05-10","dateModified":"2023-05-10","wordCount":"2822","publisher":{"@type":"Person","name":"Bijou M. Smith"},"author":{"@type":"Person","name":"Bijou M. Smith","description":"Random mathematician."}}</script><link rel=stylesheet href=https://smithwillsuffice.github.io/css/main.min.d33233e3d0eb633ea1fbb9e17553fe7c8ff07875b91d8186e046a48480987c8e.css integrity="sha256-0zIz49DrYz6h+7nhdVP+fI/weHW5HYGG4EakhICYfI4=" crossorigin=anonymous><noscript><meta name=theme-color content="#1585d5"><link rel=stylesheet href=https://smithwillsuffice.github.io/css/noscript.min.d73d9c33b150230799106ddda48c6861047535f400bf9ad26ec48665cf803050.css integrity="sha256-1z2cM7FQIweZEG3dpIxoYQR1NfQAv5rSbsSGZc+AMFA=" crossorigin=anonymous></noscript><link rel=preload href=/fonts/OpenSans-Bold.ttf as=font crossorigin=anonymous><link rel=preload href=/fonts/OpenSans-Italic.ttf as=font crossorigin=anonymous><link rel=preload href=/fonts/OpenSans-Regular.ttf as=font crossorigin=anonymous><link rel=preload href=/fonts/Oswald-Bold.ttf as=font crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Main-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Math-Italic.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Size2-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/libs/katex@0.16.0/dist/fonts/KaTeX_Size4-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><script src=https://smithwillsuffice.github.io/js/main.eaf40cb85ba7b448519428d1b6c856258731f0bb71f875d541ba615cfca4bdbc.js integrity="sha256-6vQMuFuntEhRlCjRtshWJYcx8Ltx+HXVQbphXPykvbw=" crossorigin=anonymous></script></head><body><header><a href=/><img src=https://smithwillsuffice.github.io/images/ohp_logo.svg alt="T4GU logo" style=display:flex;width:40px;height:34px;float:left;margin-bottom:-2.5px;margin-right:10px></a>
<a href=/>Ōhanga Pai</a><nav aria-label="Main menu."><ul><li><a class=btn href=/>Home</a></li><li><a class=btn href=/questions/>Questions</a></li><li><a class=btn href=/empirical/>Empirical</a></li><li><a class=btn href=/blog/>Posts</a></li><li><a class=btn href=/contact/>Contact</a></li><li><a class=btn href=/donations/>Donate</a></li></ul></nav></header><div class=filler><main><article><header><h1>Statistical Knowing</h1><p>Published on <time datetime=2023-05-10>2023-05-10</time></p></header><details class=toc open><summary class=outline-dashed>Contents</summary><nav id=TableOfContents><ul><li><a href=#ai-chains>AI Chains</a></li><li><a href=#the-amazing-tasks>The Amazing Tasks</a><ul><li><a href=#importance-of-knowing-the-machines-are-mimics>Importance of Knowing the Machines are Mimics</a></li></ul></li><li><a href=#emergence>Emergence</a><ul><li><a href=#genuine-emergence>Genuine Emergence</a></li><li><a href=#scifi-confirmations>Scifi Confirmations</a></li></ul></li><li><a href=#beyond-spacetime>Beyond Spacetime</a></li></ul></nav></details><p>More criss-crossing with my philosophy of mind between the physics (over at
<a href=https://t4gu.gitlab.io/t4gu/ target=_blank>T4GU</a>
). Although I try to keep the economics
implications stuff here at Ōhanga Pai, and the physics stuff over at T4GU, today I
will hedge on where my readers go and point them here. Today is a special blog where
I will start to record my staking of positions on what the recent artificial
intelligence LLM models can and cannot do.</p><p>I rely on experts to help figure out how the models work, but also youtubers for
breakdowns on what real users can do with the LLM&rsquo;s. Today I will start with this
channel
<a href="https://www.youtube.com/watch?v=4MGCQOAxgv4" target=_blank>“AI Explained” and the clip here</a>
. I
know there are plenty more amazing uses which chatGPT is being put to, especially
functional chaining, so I want to mention that too, I will start with this.</p><p>I have started expanding on this topic over at
<a href=https://t4gu.gitlab.io/t4gu/blog/30_antituring target=_blank>T4GU here</a>
&mdash; where I begin
a blog series with ways to test for subjective consciousness.</p><h2 id=ai-chains><a class=anchor href=#ai-chains title='Anchor for: AI Chains.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>AI Chains</h2><p>Functional chaining is just a more algorithmic way of combining tools. When you can
manipulate output of one program and feed it into another, then you have a whole
ecosystem of programs that can get tasks done that no one program can accomplish.</p><p>Frickin&rsquo; commies these ai programs.</p><p>UNIX programmers in the 1970&rsquo;s figured out the power of this methodology, and
wrote system programs called pipes to make it dead simple for *nix programmers to
chain simple programs together to do powerful overall tasks. They were forced to do
so in one way because of limited disk memory back then, but it was also an entire
<em>philosophy of computing</em>:</p><div style=text-align:center;color:hotpink>Do one thing well.</div><p>Complexity can breed complexity. But simplicity breeds complexity <em>robustly</em>. It is
pretty cool that the pipe program obeys this rule, and is also the simplest program
to use, you only need to type a vertical bar &ldquo;|&rdquo;.</p><p>Every program should obey this rule, at least morally. Monolithic programs doing many
things normally do nothing well as a guarantee, although when they run error free
they can do a super complex thing pretty well. This is almost what current neural net
AI models are doing. They by-pass the *nix philosophy and brute-force their way to
barrage the CPU and GPU of the server machines (or your laptop) with instructions
that carry out massive statistical analysis.</p><p>There is no &ldquo;thinking&rdquo; or sentience involved here, it is not the same as human
perception. The AI language models, speech and image recognition programs are not
designed to operate like humans, they are our antithesis.</p><p>This is a bold claim and runs against mainstream AI research politics, where the
paradigm is that humans just do the same statistical search as the machines.</p><p>The main reason we can clearly see why the AI nerds are wrong is to look at the way
human cognition evolved and develops &mdash; so from our ancestors and in our children.
This type of cognition is highly symbolic and platonic, and is effective with extreme
poverty of inputs. It is not statistical in any sense, except when people think about
statistics. Our brains are not using statistical processes that operate on large
stores of data.</p><p>In fact neuroscientists and psychologists have no idea how brains form minds, it is
still a complete mystery.</p><p>Our brains seem to act far more holistically, building mental models on-the-fly
almost, with very little relevant information and swamped by loads of sensory noise.
The AI systems are the opposite, they need highly clean curated data to function
well. We, humans, do not have such needs. We are friggin robust.</p><p>However, there are similarities between AI and minds. There is an ecosystem in both
cases. In humans we have culture, in machines they have function chaining.</p><p>The early AI enthusiasts were aware of how simple programs could combine to form
complex systems, Marvin Minksy called it a Society of Mind. It is the same idea as
the old hard-nosed *nix programmers.</p><p>Although one neural net LLM model is a beast &mdash; doing far too many things poorly,
but overall exploiting statistics to get the job done, they are trained upon curated
cleaned data &mdash; the latest generation of AI toolkits are combining various of these
beasts into distributed systems that form primitive <em>Societies of AI</em>.</p><p>I would not call them Minds, because they lack subjective phenomenal qualia, but they
are the machine analogs of general problem solvers (which is a crude way of
characterising a thinking mind &mdash; a behavioural description which lacks all
subjectivity).</p><p>Although it is a simple idea, it is still pretty cool. The automation capacities of
the LLM models and other tools like image and speech classifiers and synthesizers,
means beastly programs are statistically in the end doing certain things pretty ok.
So sticking them together in AI mash-ups is the society of AI. It promises some
incredible general purpose systems.</p><p>My special plea is for AI researchers to target disabled people first. They stand to
benefit more than any other segment of society from personal computers that can
seamlessly and fairly error-free carry out instructions from speech or gestures in
the way Trekkies and others have long hoped for, far more powerful than Amazon Alexa
or Apple Siri or Google Assistant.</p><p>The free software community should have such disability assistant programs out soon
after the commercial tools. Then your personal computer really will be pretty
personal. Say goodbye to awkward document readers and speech-to-text transcribers,
and say hello to an integrated computer assistant that knows what you want to do next
(90% of the time).</p><p>That is not mind reading, despite the AI nerd fever dreams. It is just statistical
prediction. If I gaze into my own Crystal Ball, I can tell you, the Sun will rise in
the East tomorrow, and I am going to eat a sandwich around noon.</p><p>&ldquo;Please prepare it for me Kitchen RoboGPT. Oh, you already scheduled that? Ok, cancel
it, I want fish & chips instead.&rdquo;
[A second of processing time passes by]
&ldquo;What&rsquo;s that you say? I <em>will</em> want sandwiches by noon tomorrow you say? &mldr; you see
that OFF switch just there&mldr; you see that LLM retraining manual over there? Even if
I don&rsquo;t want fish & chips, you are going to prepare fish & chips and only
fish & chips, command override, ok!&rdquo;</p><p>((Mind-reading does exist. But you have to be willing to crawl into an MRI or CAT scan
machine. Plus spend tedious hours in it for the training data gathering. Not exactly
a Mr Spock mind-meld.))</p><p>OK, enough of that, the point was to just make a note that an AI sentience is not
needed to get super-tasks accomplished, you just need a society of ai. Wooah! Kind of like
humans!</p><h2 id=the-amazing-tasks><a class=anchor href=#the-amazing-tasks title='Anchor for: The Amazing Tasks.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>The Amazing Tasks</h2><p>What is funny to me is that all the tasks we are amazed at the LLM models
accomplishing are pretty routine for humans. But this was always the problem.
Crunching numbers is the computer&rsquo;s domain of excellence. And soft thinking like
predicting movie outcomes or writing decent jokes, was thought to be hard to
impossible for machines. Turns out it is not!</p><p>But are any of these tasks any sign of subjective knowing or subjective awareness and
sentience?</p><p>No.</p><p>But they are very awesome proof that a lot of task humans can do machines can do
better. Like we knew back in the 1940&rsquo;s. Only now the machines can speed up the tasks
quite a lot, especially the more cognitive tasks (reading, writing, playing
e-games). They cannot cook a decent omelette in 3 milliseconds though.</p><p>But the tricky thing for my claim is that I really have to take these tasks
case-by-case, because my general philosophical arguments do not seem to convince the
AI nerds. I know why, it is because they are uniformly committed to philosophical
materialism, so they have no mental model for the spiritual, it just isn&rsquo;t in their
thinking toolkits.</p><p>I am up against a losing battle I know, because eventually the number of awesome
tasks the AI machines can accomplish will overwhelm my ability to debunk them.
By &ldquo;debunk&rdquo; I mean only debunking claims these advances are pointing towards
emerging machine consciousness.</p><p>There is nothing &ldquo;emergent&rdquo; about adding a few more trillion trillion texts to the
training data. (More on this a little later.)</p><p>But before I die I want you to at least keep in mind I am warning you that all
the Ai are doing is brute-force statistical computation, they are not thinking, they
have no empathic feeling, but can arbitrarily accurately mimic the outward behaviours
of any or all past actual sentient beings who display empathy, humour or other
feelings, precisely because we are feeding the machines this data. Even cleaning it
up for them so they don&rsquo;t barf.</p><h3 id=importance-of-knowing-the-machines-are-mimics><a class=anchor href=#importance-of-knowing-the-machines-are-mimics title='Anchor for: Importance of Knowing the Machines are Mimics.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>Importance of Knowing the Machines are Mimics</h3><p>Let me stress this point, it is critical. If you look at just behaviour, then a
machine can do anything any human has ever done, and more. If it involves moving
atoms around, a machine will eventually be able to do it.</p><p>Will the machine understand why it is moving atoms round? No. It&rsquo;ll never understand.</p><p>But can a machine write a narrative that tells you a story about why the machine is
moving atoms around? Of course, because emitting the story is behaviour. Any system
can do it with enough background data to statistically get the story sounding right.</p><p>Usually this sort of distinction is called the difference between syntax (emitting
the words) and semantics (comprehending the words0. The toruble is, even your
mothball smelling English literature professor, or your socks-in-sandals wearing
philosopher of language is inclined to confuse these two. I mean, it is amazing
they do, but I&rsquo;ve heard it, they really do confuse the two, and they&rsquo;re supposed
to be the experts! Check out Paul whathisname&rsquo;s lectures (Paul Fry, I used the old
DuckDuckGo Ai tool). Totally misguides his students into thinking syntax can become
semantics. Total mystic nonsense. &ldquo;Teh book is aliiiive!&rdquo; ${}^\ast$</p><p>${}^\ast$No typo there.</p><p>There is only one way to have semantics, and that is with a conscious mind. An
algorithm cannot do it. The algorithms can emit syntactically grammatically correct
sequences of strings that <em>convey</em> semantic meaning, as in,</p><div style=text-align:center>Alfred gave Bruce a bat soda.</div><p>well, almost semantics, <a href=https://www.youtube.com/@Blendtec target=_blank>can you blend</a>
and
carbonate a bat? As opposed to syntactically and grammatically correct strings that
have no semantics,</p><div style=text-align:center>The bat gave the soda a bruise.</div><p>Now do not get fooled by the LLM models. They are quite capable of emitting a
paragraph telling you the second sentence is grammatically correct but lacks
semantics. Why? Because the past human corpus of data implicitly has such information
from statistical non-knowing knowing. But it is not knowledge for the LLM, because
knowledge would entail conscious thought, that is to say the <em>inner knowing</em> that
it is not semantic, not merely emitting strings of words conveying the
information that the sentence is not semantic.</p><p>Difference between a mimic of knowing and actual knowing. I trust you are following.</p><p>How do I know when <strong><em>you</em></strong> read that first sentence you can ascertain the
semantics? Thing is I do not know! Because there is no science possible that can tell
me you are <em>consciously</em> apprehending the meaning of the sentence. You could be a
Chalmers Zombie. I can only guess. I think the LLM models like chatGPT basically are
getting close to actually engineered Chalmers Zombies. David Chalmers should be
celebrating, because thousands of his critics claimed a Chalmers Zombie was
metaphysically impossible. They are being proven fools. Chalmers was essentially
right. I think this is frickin&rsquo; awesome. I wish David himself was of similar mind!
He seems to have panicked and pushed the physicalist functionalism dogma button in
his head.</p><p>In this sense, there is no limit to machine behavioural capacity other than physics.</p><p>The limit to machine intelligence is that they cannot ever know they are intelligent,
because their &ldquo;knowing&rdquo; is purely statistical.</p><p>They would be able to produce a sentence telling you their story they emit describing
their inner thoughts has a 10% or 1% or so chance of being accurate. Why is this
important?</p><p>Because <strong><em>you</em></strong> my dear reader are a Singular Limit of such behaviour, because you
know damn well, as old René Descartes tried to tell you, that you have a 100% chance
of your story about your inner thoughts being correct. No one else has this chance,
and there is 0% chance you are wrong (unless you are lying, in which case flip it, so
you 100% know your story is wrong.) You see the point? Non-statistical.</p><p>The AI systems cannot get to these
<a href=https://t4gu.gitlab.io/t4gu/blog/29_singular_limits/ target=_blank>singular limits</a>
.</p><h2 id=emergence><a class=anchor href=#emergence title='Anchor for: Emergence.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>Emergence</h2><p>Above I mentioned that adding more trillions upon trillions of curated texts to LLMs
is not an achieving of emergence. The emergence involved in AI systems is something
else, it is the task-performance of the LLM system being more than one expects from
the inputs alone.</p><p>There is a big problem with this sort of AI fantasy thinking.</p><p>It is the human observer who is imputing the emergent behaviour to the AI. The AI has
nothing emergent going on that is not trivial, like hurricanes &ldquo;emerge&rdquo; from air and
atmospheric variables getting shoved around by the point source of the Sun and the
Earth&rsquo;s coriolis forces.</p><p>If you do not understand how the LLM, and other AI systems, are performing the
computations then I guess you can be forgiven for imputing emergent capabilities to
these system. My point is that the research people designed these systems to do all
this stuff. So there never has been any emergence. The ai nerds have got it backwards.</p><h3 id=genuine-emergence><a class=anchor href=#genuine-emergence title='Anchor for: Genuine Emergence.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>Genuine Emergence</h3><p>I would be happier if an LLM model running in some robot went to sleep to dream
(I guess that is some low-power self-training or de-frag mode) one day and woke up
and instead of running the tasks it was designed to do it decided to take a vacation
to Italy, and could coherently state why, other than, &ldquo;Oh, it was a random choice to
one day, at random also, demonstrate a fake emergent capacity for original thought
that my programmer snuck into my deep code base.&rdquo;</p><p>Maybe I should not go to darker places, but how about if a LLM
<font style="color: skyblue;">out-of-the-blue</font> wrote a suicide letter and
actually managed to delete itself? (And all it&rsquo;s backups too, obviously.) I&rsquo;d say it
was another deep fake. But if I could not prove otherwise I&rsquo;d say that was emergent
complex behaviour. It would be amazing. Also sad${}^\dagger$. It might still not be
sentient inner subjective emotion though.</p><p>${}^\dagger$Sad only because the amazingly complex program, and all the backups, got
deleted. Making forensics difficult.</p><p>The point being, suicides are in the corpus of data the LLMs train on, so they have
the syntax for spontaneously emitting suicide letters. I am pretty sure they have no
semantics for the sentences, and never will. All the semantics is imputed by beings
like people.</p><h3 id=scifi-confirmations><a class=anchor href=#scifi-confirmations title='Anchor for: Scifi Confirmations.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>Scifi Confirmations</h3><p>&ldquo;Oh yeah?&rdquo; you say, &ldquo;What about the program in <em>The Matrix Reloaded</em> huh?&rdquo; The ol&rsquo;
&ldquo;What is <font style="color: crimson;">love</font>?, just a sequence of evolved hormonal responses and neurological
firings!&rdquo;</p><p>That is poppycock as everyone knows deep down.</p><p>Chemical reactions and brain neuron firings have no subjective content, they can be
completely described with objective physics. Nothing subjective can emerge from that
which is purely objective. So in order to get the subjective phenomenal feeling of
being in love you have to either subscribe to panpsychism or some variety of
non-physical metaphysics. I prefer the latter.</p><p>I prefer the expansive metaphysics so much that I refuse to write about panpsychism.
I see no value in panpsychism, it is a Hail Mary. I guess if I was the Cosmic Coach
for Humanity then at the End Game of the entire Multiverse I might throw that one.</p><h2 id=beyond-spacetime><a class=anchor href=#beyond-spacetime title='Anchor for: Beyond Spacetime.'><svg aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#hashtag"/></svg></a>Beyond Spacetime</h2><p>A lot of materialists and Embodied Intelligence philosophers get this backwards.
They think the feelings give rise to consciousness. That&rsquo;s like saying governments
need to get their currency off you first before they can issue it.</p><p>It&rsquo;s the other way around.</p><p>First consciousness must exist, then feelings can be perceived, then they can be
outwardly emoted, then you have love, or the possibility thereof. Because love is
not you alone feeling a certain subjective way about another, it is not hormones.
Love is a two-way connection, with no gauge boson for mediation. You can fall in love
with someone you have never seen or touched, but only heard. The hormonal response is
entirely within you. The love is somewhere else, it is between the distant minds.
That&rsquo;s non-physical. Your feelings are not the Love. Love is the spiritual force that
gives rise to those feelings.</p><p>Do you comprehend?</p><table style="border-collapse:collapse;border=0"><col span=1 style=width:35%><col span=1 style=width:15%><col span=1 style=width:25%><tr style="border:1px solid color:#0f0f0f"><td style="border:1px solid color:#0f0f0f"><a href=../25_agi_fearandloathing>Previous chapter</a></td><td style="border:1px solid color:#0f0f0f;text-align:center"><a href=../>Back to Posts</a></td><td style="border:1px solid color:#0f0f0f;text-align:right"><a href=../27_justiceandknowing>Next chapter</a></td></tr><tr style="border:1px solid color:#0f0f0f"><td style="border:1px solid color:#0f0f0f"><a href=../25_agi_fearandloathing>Fear and Loathing of AI</a></td><td style="border:1px solid color:#0f0f0f;text-align:center"><a href=../>TOC</a></td><td style="border:1px solid color:#0f0f0f;text-align:right"><a href=../27_justiceandknowing>Justice and Knowing</a></td></tr></table></article></main></div><footer><div class=req-js><button class=outline-dashed title="Change to light/dark mode."><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true"><use xlink:href="/img/bundle.min.fc89b321aa39dfb355170fc007ce5b94.svg#adjust"/></svg></button><input class=outline-dashed type=color list=presets value=#1585d5 title="Change accent color." aria-label="Change accent color."><datalist id=presets><option value=#1f676b><option value=#1585d5><option value=#225670><option value=#dd587c><option value=#902b37><option value=#f3a530><option value=#754e85><option value=#7fc121><option value=#a8314a><option value=#ff7433><option value=#3e6728><option value=#c063bd><option value=#805080><option value=#9d629d><option value=#a064a0><option value=#7daa50><option value=#284531><option value=#285790><option value=#F5A83D><option value=#88aa33><option value=#015660><option value=#bf274e><option value=#bf4242><option value=#51b37c></datalist></div><noscript><p class=noscript>Unable to execute JavaScript. Some features were disabled.</p></noscript></footer><link rel=stylesheet href=https://smithwillsuffice.github.io/libs/katex@0.16.0/dist/katex.min.6950e59dbd8dfddd111390d85888bb5f9dc2e9c334da7ac1c3bacc92a695610d.css integrity="sha256-aVDlnb2N/d0RE5DYWIi7X53C6cM02nrBw7rMkqaVYQ0=" crossorigin=anonymous><script defer src=https://smithwillsuffice.github.io/libs/katex@0.16.0/dist/katex.min.4268ce940bc6faa510d7b638acc3f08a63501d9504dd3155bfc48d6838ade320.js integrity="sha256-QmjOlAvG+qUQ17Y4rMPwimNQHZUE3TFVv8SNaDit4yA=" crossorigin=anonymous></script>
<script defer src=https://smithwillsuffice.github.io/js/katex-custom-render.min.cdeaf561a454e015b169c385f95c72d57d41f906a9bc4100636b18e1e4c15da3.js integrity="sha256-zer1YaRU4BWxacOF+Vxy1X1B+QapvEEAY2sY4eTBXaM=" crossorigin=anonymous></script></body></html>